# main_advanced.py - Vers√£o avan√ßada com conex√£o PostgreSQL

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√µes
from config import *
from technical_indicators import add_technical_indicators, add_lagged_features, add_rolling_statistics

# Importa√ß√£o do DatabaseManager com tratamento de erro
try:
    from database import# Fun√ß√£o utilit√°ria para monitoramento
def show_system_stats():
    """
    Mostra estat√≠sticas b√°sicas do sistema durante processamento
    """
    import os
    import gc
    
    # For√ßa coleta de lixo
    gc.collect()
    
    # Informa√ß√µes b√°sicas
    print(f"   üêç PID do processo: {os.getpid()}")
    print(f"   üìä Status: Processando...")

# ...existing code...  DATABASE_AVAILABLE = True
    print("‚úÖ DatabaseManager carregado com sucesso")
except ImportError as e:
    print(f"‚ö†Ô∏è DatabaseManager n√£o dispon√≠vel: {e}")
    DATABASE_AVAILABLE = False
    DatabaseManager = None
except Exception as e:
    print(f"‚ö†Ô∏è Erro ao carregar DatabaseManager: {e}")
    DATABASE_AVAILABLE = False
    DatabaseManager = None

# Imports do TensorFlow (com tratamento de erro e configura√ß√£o para macOS)
try:
    import os
    # Configura√ß√µes espec√≠ficas para resolver problemas de mutex/threading no macOS
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduz logs do TensorFlow
    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Desabilita oneDNN que pode causar problemas
    os.environ['OMP_NUM_THREADS'] = '1'  # Limita threads OpenMP
    os.environ['TF_NUM_INTEROP_THREADS'] = '1'  # Limita threads de interopera√ß√£o
    os.environ['TF_NUM_INTRAOP_THREADS'] = '1'  # Limita threads de opera√ß√£o interna
    
    import tensorflow as tf
    
    # Configura√ß√£o adicional do TensorFlow para macOS
    tf.config.threading.set_inter_op_parallelism_threads(1)
    tf.config.threading.set_intra_op_parallelism_threads(1)
    
    # Configurar GPUs se dispon√≠veis (mas limitar problemas de threading)
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(f"Aviso: N√£o foi poss√≠vel configurar GPU: {e}")
    
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    from tensorflow.keras.optimizers import Adam
    TENSORFLOW_AVAILABLE = True
    print("‚úÖ TensorFlow carregado com configura√ß√µes otimizadas para macOS")
except ImportError as e:
    print(f"TensorFlow n√£o dispon√≠vel: {e}. Apenas Random Forest ser√° usado.")
    TENSORFLOW_AVAILABLE = False
except Exception as e:
    print(f"Erro ao configurar TensorFlow: {e}. Tentando modo simplificado...")
    try:
        import tensorflow as tf
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
        from tensorflow.keras.optimizers import Adam
        TENSORFLOW_AVAILABLE = True
        print("‚úÖ TensorFlow carregado em modo simplificado")
    except:
        print("‚ùå TensorFlow completamente indispon√≠vel. Apenas Random Forest ser√° usado.")
        TENSORFLOW_AVAILABLE = False

# Imports do sklearn
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit

def load_and_preprocess_advanced_data_from_db(config, limit=None):
    """
    Carregamento e pr√©-processamento avan√ßado dos dados do PostgreSQL
    """
    if not DATABASE_AVAILABLE:
        raise Exception("DatabaseManager n√£o est√° dispon√≠vel - problema de compatibilidade com typing_extensions/SQLAlchemy")
    
    print("=== CARREGANDO DADOS DO POSTGRESQL ===")
    
    # Inicializa conex√£o com banco
    db = DatabaseManager()
    
    try:
        # Conecta ao banco
        if not db.connect():
            raise Exception("Falha ao conectar com o banco de dados")
        
        # Obt√©m informa√ß√µes da tabela
        print("Verificando estrutura da tabela...")
        db.get_table_info('botbinance')
        
        # Carrega os dados
        print("Carregando dados da tabela botbinance...")
        df = db.load_botbinance_data(limit=limit, order_by='id')
        
        if df is None or len(df) == 0:
            raise Exception("Nenhum dado encontrado na tabela botbinance")
        
        print(f"‚úÖ Dados carregados do banco: {df.shape}")
        print(f"Colunas dispon√≠veis: {list(df.columns)}")
        
        # Verifica se as colunas necess√°rias existem
        required_columns = ['close', 'open', 'high', 'low']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"‚ö†Ô∏è Colunas obrigat√≥rias n√£o encontradas: {missing_columns}")
            # Tenta mapear colunas com nomes similares
            column_mapping = {}
            for col in missing_columns:
                similar_cols = [c for c in df.columns if col.lower() in c.lower()]
                if similar_cols:
                    column_mapping[similar_cols[0]] = col
                    print(f"Mapeando {similar_cols[0]} -> {col}")
            
            if column_mapping:
                df = df.rename(columns=column_mapping)
        
        # Adiciona indicadores t√©cnicos
        print("Calculando indicadores t√©cnicos...")
        df = add_technical_indicators(df, TECHNICAL_FEATURES)
        
        # Adiciona features com lag
        print("Adicionando features com lag...")
        base_columns = ['close', 'open', 'high', 'low']
        if 'volume' in df.columns:
            base_columns.append('volume')
        df = add_lagged_features(df, base_columns, lags=[1, 2, 3, 5])
        
        # Adiciona estat√≠sticas rolantes
        print("Calculando estat√≠sticas rolantes...")
        df = add_rolling_statistics(df, ['close'], windows=[5, 10, 20])
        
        # Remove NaN
        initial_rows = len(df)
        df = df.dropna()
        removed_rows = initial_rows - len(df)
        print(f"Removidas {removed_rows} linhas com NaN")
        print(f"Dataset final: {df.shape}")
        
        # Seleciona features para o modelo (exclui colunas de ID e timestamp)
        exclude_columns = ['id', 'created_at', 'updated_at', 'timestamp']
        feature_columns = [col for col in df.columns if col not in exclude_columns]
        
        print(f"Features selecionadas: {len(feature_columns)}")
        print(f"Primeiras 10 features: {feature_columns[:10]}")
        
        return df, feature_columns
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar dados do banco: {e}")
        raise
    finally:
        # Sempre fecha a conex√£o
        db.disconnect()

def load_and_preprocess_advanced_data_from_csv(file_path, config):
    """
    Fun√ß√£o original para carregar dados do CSV (mantida como fallback)
    """
    print("=== CARREGANDO DADOS DO CSV (FALLBACK) ===")
    df = pd.read_csv(file_path, delimiter=';')
    
    print(f"Dataset original: {df.shape}")
    print(f"Colunas dispon√≠veis: {list(df.columns)}")
    
    # Adiciona indicadores t√©cnicos
    print("Calculando indicadores t√©cnicos...")
    df = add_technical_indicators(df, TECHNICAL_FEATURES)
    
    # Adiciona features com lag
    print("Adicionando features com lag...")
    base_columns = ['close', 'open', 'high', 'low']
    if 'volume' in df.columns:
        base_columns.append('volume')
    df = add_lagged_features(df, base_columns, lags=[1, 2, 3, 5])
    
    # Adiciona estat√≠sticas rolantes
    print("Calculando estat√≠sticas rolantes...")
    df = add_rolling_statistics(df, ['close'], windows=[5, 10, 20])
    
    # Remove NaN
    df = df.dropna()
    print(f"Dataset ap√≥s processamento: {df.shape}")
    
    # Seleciona features para o modelo
    feature_columns = [col for col in df.columns if col not in ['id', 'created_at']]
    
    print(f"Features selecionadas: {len(feature_columns)}")
    
    return df, feature_columns

def create_sequences_advanced(data, target_col_idx, seq_length):
    """
    Cria sequ√™ncias para modelos de s√©ries temporais
    """
    X, y = [], []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i])
        y.append(data[i, target_col_idx])
    
    return np.array(X), np.array(y)

def train_advanced_lstm(X_train, y_train, X_test, y_test, config):
    """
    Treina modelo LSTM com arquitetura avan√ßada
    """
    if not TENSORFLOW_AVAILABLE:
        print("TensorFlow n√£o dispon√≠vel, pulando LSTM...")
        return None, None
    
    print("Construindo modelo LSTM avan√ßado...")
    
    model = Sequential()
    
    # Primeira camada LSTM
    model.add(LSTM(
        config['layers'][0], 
        return_sequences=True, 
        input_shape=(X_train.shape[1], X_train.shape[2])
    ))
    model.add(BatchNormalization())
    model.add(Dropout(config['dropout_rates'][0]))
    
    # Camadas LSTM intermedi√°rias
    for i in range(1, len(config['layers'])-1):
        model.add(LSTM(config['layers'][i], return_sequences=True))
        model.add(BatchNormalization())
        model.add(Dropout(config['dropout_rates'][i]))
    
    # √öltima camada LSTM
    model.add(LSTM(config['layers'][-1]))
    model.add(BatchNormalization())
    model.add(Dropout(config['dropout_rates'][-1]))
    
    # Camadas densas
    for dense_size in config['dense_layers']:
        model.add(Dense(dense_size, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.2))
    
    # Camada de sa√≠da
    model.add(Dense(1))
    
    # Compila√ß√£o
    optimizer = Adam(learning_rate=config['learning_rate'])
    model.compile(optimizer=optimizer, loss=config['loss_function'], metrics=['mae'])
    
    print("Arquitetura do modelo:")
    model.summary()
    
    # Callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=config['patience_early_stop'],
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=config['patience_reduce_lr'],
            min_lr=0.0001,
            verbose=1
        ),
        ModelCheckpoint(
            'models/best_lstm_model.h5',  # Mudando para formato .h5 para compatibilidade
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # Treinamento
    print("Iniciando treinamento do LSTM...")
    history = model.fit(
        X_train, y_train,
        epochs=config['epochs'],
        batch_size=config['batch_size'],
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    return model, history

def train_ensemble_models(X_train, y_train, quick_mode=False):
    """
    Treina m√∫ltiplos modelos para ensemble
    """
    import time
    import threading
    import sys
    from datetime import datetime
    
    X_train_flat = X_train.reshape(X_train.shape[0], -1)
    print(f"üìä Dados achatados para modelos tradicionais: {X_train_flat.shape}")
    
    models = {}
    
    # Configura par√¢metros baseado no modo
    if quick_mode:
        print("‚ö° MODO R√ÅPIDO ATIVADO - Menos itera√ß√µes para teste")
        rf_iterations = 10  # Reduzido de 200
        rf_cv_folds = 3     # Reduzido de 7
        gb_iterations = 5   # Reduzido de 30
        gb_cv_folds = 2     # Reduzido de 3
    else:
        print("üéØ MODO COMPLETO - Busca completa de hiperpar√¢metros")
        rf_iterations = RF_CONFIG['random_search_iterations']
        rf_cv_folds = RF_CONFIG['cv_folds']
        gb_iterations = 30
        gb_cv_folds = 3
    
    # Random Forest
    print(f"\nüå≤ === INICIANDO TREINAMENTO RANDOM FOREST ===")
    print(f"‚è∞ In√≠cio: {datetime.now().strftime('%H:%M:%S')}")
    print(f"üîß Modo: {'R√ÅPIDO' if quick_mode else 'COMPLETO'}")
    
    rf_param_dist = {
        'n_estimators': RF_CONFIG['n_estimators_options'],
        'max_depth': RF_CONFIG['max_depth_options'],
        'min_samples_split': RF_CONFIG['min_samples_split_options'],
        'min_samples_leaf': RF_CONFIG['min_samples_leaf_options'],
        'max_features': RF_CONFIG['max_features_options']
    }
    
    print(f"üîß Par√¢metros para busca:")
    for param, values in rf_param_dist.items():
        print(f"   {param}: {values}")
    
    total_combinations = rf_iterations * rf_cv_folds
    print(f"üîç Testando {rf_iterations} combina√ß√µes com {rf_cv_folds} folds")
    print(f"üìà Total de fits: {total_combinations}")
    estimated_time = total_combinations * 2  # Estimativa de 2 segundos por fit
    print(f"‚è±Ô∏è  Tempo estimado: {estimated_time/60:.1f} minutos")
    print("üí° Aguarde o processamento...")
    
    start_time = time.time()
    
    rf = RandomForestRegressor(random_state=42, n_jobs=-1)
    rf_search = RandomizedSearchCV(
        rf, rf_param_dist, 
        n_iter=rf_iterations,
        cv=TimeSeriesSplit(n_splits=rf_cv_folds),
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        verbose=2  # Aumentado para mais detalhes
    )
    
    print("üöÄ Iniciando busca de hiperpar√¢metros...")
    
    # Cria uma fun√ß√£o de callback para mostrar progresso
    class ProgressCallback:
        def __init__(self):
            self.start_time = time.time()
            self.last_update = time.time()
        
        def __call__(self, *args):
            current_time = time.time()
            if current_time - self.last_update > 30:  # Atualiza a cada 30 segundos
                elapsed = current_time - self.start_time
                print(f"   ‚è±Ô∏è  Processando h√° {elapsed:.0f}s ({elapsed/60:.1f} min)... ainda trabalhando...")
                self.last_update = current_time
    
    progress_callback = ProgressCallback()
    
    # Inicia um timer para mostrar progresso periodicamente
    import threading
    import sys
    
    def progress_monitor():
        start = time.time()
        while not hasattr(progress_monitor, 'stop'):
            time.sleep(45)  # Aguarda 45 segundos
            if not hasattr(progress_monitor, 'stop'):
                elapsed = time.time() - start
                print(f"   üîÑ Random Forest ainda processando... {elapsed:.0f}s ({elapsed/60:.1f} min)")
                sys.stdout.flush()
    
    # Inicia monitor em thread separada
    monitor_thread = threading.Thread(target=progress_monitor, daemon=True)
    monitor_thread.start()
    
    try:
        rf_search.fit(X_train_flat, y_train)
    finally:
        # Para o monitor
        progress_monitor.stop = True
    
    rf_time = time.time() - start_time
    print(f"‚úÖ Random Forest conclu√≠do!")
    print(f"‚è±Ô∏è  Tempo decorrido: {rf_time:.1f} segundos ({rf_time/60:.1f} minutos)")
    print(f"üèÜ Melhor score: {rf_search.best_score_:.6f}")
    print(f"üîß Melhores par√¢metros:")
    for param, value in rf_search.best_params_.items():
        print(f"   {param}: {value}")
    
    models['rf'] = rf_search.best_estimator_
    
    # Gradient Boosting
    print("\nüöÄ === INICIANDO TREINAMENTO GRADIENT BOOSTING ===")
    print(f"‚è∞ In√≠cio: {datetime.now().strftime('%H:%M:%S')}")
    
    gb_param_dist = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 0.9, 1.0],
        'max_features': ['sqrt', 'log2', None]
    }
    
    print(f"üîß Par√¢metros para busca:")
    for param, values in gb_param_dist.items():
        print(f"   {param}: {values}")
    
    gb_total_combinations = gb_iterations * gb_cv_folds  # n_iter * cv_folds
    print(f"üîç Testando {gb_iterations} combina√ß√µes com {gb_cv_folds} folds")
    print(f"üìà Total de fits: {gb_total_combinations}")
    gb_estimated_time = gb_total_combinations * 3  # Estimativa de 3 segundos por fit
    print(f"‚è±Ô∏è  Tempo estimado: {gb_estimated_time/60:.1f} minutos")
    print("üí° Processando Gradient Boosting...")
    
    start_time = time.time()
    
    gb = GradientBoostingRegressor(random_state=42)
    gb_search = RandomizedSearchCV(
        gb, gb_param_dist,
        n_iter=gb_iterations,
        cv=TimeSeriesSplit(n_splits=gb_cv_folds),
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        verbose=2  # Aumentado para mais detalhes
    )
    
    print("üöÄ Iniciando busca de hiperpar√¢metros GB...")
    
    # Monitor de progresso para GB
    def progress_monitor_gb():
        start = time.time()
        while not hasattr(progress_monitor_gb, 'stop'):
            time.sleep(30)  # Aguarda 30 segundos
            if not hasattr(progress_monitor_gb, 'stop'):
                elapsed = time.time() - start
                print(f"   üîÑ Gradient Boosting ainda processando... {elapsed:.0f}s ({elapsed/60:.1f} min)")
                sys.stdout.flush()
    
    # Inicia monitor em thread separada
    monitor_thread_gb = threading.Thread(target=progress_monitor_gb, daemon=True)
    monitor_thread_gb.start()
    
    try:
        gb_search.fit(X_train_flat, y_train)
    finally:
        # Para o monitor
        progress_monitor_gb.stop = True
    
    gb_time = time.time() - start_time
    print(f"‚úÖ Gradient Boosting conclu√≠do!")
    print(f"‚è±Ô∏è  Tempo decorrido: {gb_time:.1f} segundos ({gb_time/60:.1f} minutos)")
    print(f"üèÜ Melhor score: {gb_search.best_score_:.6f}")
    print(f"üîß Melhores par√¢metros:")
    for param, value in gb_search.best_params_.items():
        print(f"   {param}: {value}")
    
    models['gb'] = gb_search.best_estimator_
    
    print(f"\nüéâ === ENSEMBLE TREINAMENTO CONCLU√çDO ===")
    print(f"‚úÖ Modelos treinados: {list(models.keys())}")
    total_time = rf_time + gb_time
    print(f"‚è±Ô∏è  Tempo total do ensemble: {total_time:.1f} segundos ({total_time/60:.1f} minutos)")
    
    return models

def calculate_comprehensive_metrics(y_true, y_pred, model_name):
    """
    Calcula m√©tricas abrangentes
    """
    metrics = {
        'Model': model_name,
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'MAE': mean_absolute_error(y_true, y_pred),
        'R¬≤': r2_score(y_true, y_pred),
        'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    }
    
    return metrics

def plot_comprehensive_results(y_test, predictions_dict, history=None):
    """
    Plota resultados abrangentes
    """
    # Configurar estilo
    plt.style.use('seaborn-v0_8')
    
    # Plot 1: Compara√ß√£o de previs√µes
    plt.figure(figsize=(20, 12))
    
    plt.subplot(2, 3, 1)
    plt.plot(y_test, label='Real', color='blue', linewidth=2)
    for name, preds in predictions_dict.items():
        plt.plot(preds, label=name, alpha=0.8)
    plt.title('Compara√ß√£o de Previs√µes')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 2: Erro por modelo
    plt.subplot(2, 3, 2)
    errors = {}
    for name, preds in predictions_dict.items():
        errors[name] = np.abs(y_test.flatten() - preds.flatten())
    
    plt.boxplot(errors.values(), labels=errors.keys())
    plt.title('Distribui√ß√£o dos Erros')
    plt.yscale('log')
    plt.xticks(rotation=45)
    
    # Plot 3: Scatter plot - Real vs Previsto
    plt.subplot(2, 3, 3)
    for name, preds in predictions_dict.items():
        plt.scatter(y_test, preds, alpha=0.6, label=name)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Valores Reais')
    plt.ylabel('Previs√µes')
    plt.title('Real vs Previsto')
    plt.legend()
    
    # Plot 4: Hist√≥rico de treinamento (se dispon√≠vel)
    if history is not None:
        plt.subplot(2, 3, 4)
        plt.plot(history.history['loss'], label='Treino')
        plt.plot(history.history['val_loss'], label='Valida√ß√£o')
        plt.title('Hist√≥rico de Perda')
        plt.legend()
        plt.yscale('log')
    
    # Plot 5: Residuos
    plt.subplot(2, 3, 5)
    for name, preds in predictions_dict.items():
        residuals = y_test.flatten() - preds.flatten()
        plt.scatter(preds, residuals, alpha=0.6, label=name)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Previs√µes')
    plt.ylabel('Res√≠duos')
    plt.title('An√°lise de Res√≠duos')
    plt.legend()
    
    # Plot 6: √öltimas previs√µes (zoom)
    plt.subplot(2, 3, 6)
    last_points = min(50, len(y_test))
    plt.plot(y_test[-last_points:], label='Real', color='blue', linewidth=2)
    for name, preds in predictions_dict.items():
        plt.plot(preds[-last_points:], label=name, alpha=0.8)
    plt.title('√öltimas 50 Previs√µes')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# Fun√ß√£o utilit√°ria para monitoramento
def show_system_stats():
    """
    Mostra estat√≠sticas do sistema durante processamento
    """
    import psutil
    import os
    
    # Informa√ß√µes de CPU e mem√≥ria
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    
    print(f"   üíª CPU: {cpu_percent:.1f}% | RAM: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)")
    
    # Informa√ß√µes do processo atual
    process = psutil.Process(os.getpid())
    process_memory = process.memory_info()
    print(f"   üêç Processo Python: {process_memory.rss/1024**2:.0f}MB RAM")

def main():
    """
    Fun√ß√£o principal - Vers√£o com PostgreSQL
    """
    import time
    from datetime import datetime
    
    print("=== SISTEMA AVAN√áADO DE PREVIS√ÉO COM POSTGRESQL ===")
    print(f"üïê In√≠cio da execu√ß√£o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    
    # Configura√ß√£o para fallback CSV
    use_database = DATABASE_AVAILABLE  # S√≥ usa database se estiver dispon√≠vel
    csv_fallback = 'relatorio_mensal_geral_2025-03 (1).csv'
    
    try:
        # Tenta carregar dados do PostgreSQL
        if use_database:
            print("Tentando carregar dados do PostgreSQL...")
            df, feature_columns = load_and_preprocess_advanced_data_from_db(
                TECHNICAL_FEATURES, 
                limit=None  # None = todos os dados, ou especifique um n√∫mero para teste
            )
        else:
            raise Exception("DatabaseManager n√£o dispon√≠vel - usando CSV")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao carregar do PostgreSQL: {e}")
        print("Tentando fallback para CSV...")
        
        try:
            df, feature_columns = load_and_preprocess_advanced_data_from_csv(
                csv_fallback, 
                TECHNICAL_FEATURES
            )
            print("‚úÖ Dados carregados do CSV com sucesso")
        except Exception as csv_error:
            print(f"‚ùå Erro tamb√©m no CSV: {csv_error}")
            print("Verifique as configura√ß√µes do banco (.env) ou a exist√™ncia do arquivo CSV")
            return
    
    # Prepara dados
    data = df[feature_columns].values
    target_col_idx = feature_columns.index('close')
    
    print(f"\nDados preparados:")
    print(f"Shape dos dados: {data.shape}")
    print(f"√çndice da coluna 'close': {target_col_idx}")
    
    # Normaliza√ß√£o
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data)
    
    # Cria sequ√™ncias
    X, y = create_sequences_advanced(data_scaled, target_col_idx, SEQ_LENGTH)
    
    # Divis√£o treino/teste
    split_idx = int(len(X) * (1 - TEST_SIZE))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"Treino: {X_train.shape}, Teste: {X_test.shape}")
    
    # Treina modelos
    predictions = {}
    metrics_list = []
    
    # LSTM
    if TENSORFLOW_AVAILABLE:
        print("\n=== TREINANDO LSTM ===")
        lstm_model, history = train_advanced_lstm(X_train, y_train, X_test, y_test, LSTM_CONFIG)
        if lstm_model is not None:
            lstm_pred = lstm_model.predict(X_test)
            
            # Desnormaliza LSTM
            lstm_pred_full = np.zeros((len(lstm_pred), len(feature_columns)))
            lstm_pred_full[:, target_col_idx] = lstm_pred.flatten()
            lstm_pred_rescaled = scaler.inverse_transform(lstm_pred_full)[:, target_col_idx]
            
            predictions['LSTM'] = lstm_pred_rescaled
    else:
        history = None
        print("‚ö†Ô∏è TensorFlow n√£o dispon√≠vel, LSTM ser√° ignorado")
    
    # Ensemble de modelos tradicionais
    print("\n=== TREINANDO MODELOS ENSEMBLE ===")
    print("ü§ñ Iniciando treinamento de modelos tradicionais (Random Forest + Gradient Boosting)")
    print("‚ö†Ô∏è  IMPORTANTE: Este processo pode levar v√°rios minutos dependendo do tamanho dos dados")
    print(f"üìä Dados de treino: {X_train.shape[0]} amostras com {X_train.shape[1]} timesteps e {X_train.shape[2]} features")
    
    # Pergunta sobre modo r√°pido
    print("\nü§î Escolha o modo de treinamento:")
    print("   1Ô∏è‚É£  COMPLETO: Busca completa de hiperpar√¢metros (mais lento, melhor precis√£o)")
    print("   2Ô∏è‚É£  R√ÅPIDO: Busca reduzida para teste (mais r√°pido, precis√£o OK)")
    
    # Por padr√£o, usar modo r√°pido se muitos dados
    use_quick_mode = len(X_train) > 5000  # Modo r√°pido para datasets grandes
    
    if use_quick_mode:
        print("üöÄ Dataset grande detectado - Usando MODO R√ÅPIDO automaticamente")
        print("   (Para for√ßar modo completo, modifique o c√≥digo)")
    else:
        print("üìä Dataset pequeno/m√©dio - Usando MODO COMPLETO")
    
    ensemble_start_time = time.time()
    ensemble_models = train_ensemble_models(X_train, y_train, quick_mode=use_quick_mode)
    ensemble_total_time = time.time() - ensemble_start_time
    
    print(f"üéØ Ensemble conclu√≠do em {ensemble_total_time:.1f} segundos ({ensemble_total_time/60:.1f} minutos)")
    
    # Desnormaliza y_test
    print("\nüìà === PREPARANDO DADOS PARA AVALIA√á√ÉO ===")
    print("üîÑ Desnormalizando dados de teste...")
    y_test_full = np.zeros((len(y_test), len(feature_columns)))
    y_test_full[:, target_col_idx] = y_test
    y_test_rescaled = scaler.inverse_transform(y_test_full)[:, target_col_idx]
    
    # Previs√µes dos modelos tradicionais
    print("\nüîÆ === FAZENDO PREVIS√ïES ===")
    X_test_flat = X_test.reshape(X_test.shape[0], -1)
    print(f"üìä Dados de teste achatados: {X_test_flat.shape}")
    
    for name, model in ensemble_models.items():
        print(f"üéØ Fazendo previs√µes com {name.upper()}...")
        pred_start = time.time()
        pred = model.predict(X_test_flat)
        pred_time = time.time() - pred_start
        
        pred_full = np.zeros((len(pred), len(feature_columns)))
        pred_full[:, target_col_idx] = pred
        pred_rescaled = scaler.inverse_transform(pred_full)[:, target_col_idx]
        predictions[name.upper()] = pred_rescaled
        
        print(f"   ‚úÖ {name.upper()} conclu√≠do em {pred_time:.2f}s")
        print(f"   üìà Previs√£o m√©dia: {pred_rescaled.mean():.2f}")
        print(f"   üìä Min: {pred_rescaled.min():.2f}, Max: {pred_rescaled.max():.2f}")
    
    # Ensemble final
    if len(predictions) > 1:
        ensemble_pred = np.mean(list(predictions.values()), axis=0)
        predictions['ENSEMBLE'] = ensemble_pred
    
    # Calcula m√©tricas
    print("\n=== CALCULANDO M√âTRICAS ===")
    for name, preds in predictions.items():
        metrics = calculate_comprehensive_metrics(y_test_rescaled, preds, name)
        metrics_list.append(metrics)
    
    # Mostra resultados
    results_df = pd.DataFrame(metrics_list)
    print("\n=== RESULTADOS FINAIS ===")
    print(results_df.round(4))
    
    # Identifica o melhor modelo
    best_model_idx = results_df['RMSE'].idxmin()
    best_model = results_df.loc[best_model_idx, 'Model']
    best_rmse = results_df.loc[best_model_idx, 'RMSE']
    print(f"\nüèÜ Melhor modelo: {best_model} (RMSE: {best_rmse:.4f})")
    
    # Previs√£o do pr√≥ximo valor
    print("\n=== PREVIS√ÉO DO PR√ìXIMO VALOR ===")
    last_sequence = data_scaled[-SEQ_LENGTH:].reshape(1, SEQ_LENGTH, -1)
    
    next_predictions = {}
    
    for name, model in ensemble_models.items():
        if name == 'rf' or name == 'gb':
            next_pred = model.predict(last_sequence.reshape(1, -1))[0]
            next_pred_full = np.zeros((1, len(feature_columns)))
            next_pred_full[0, target_col_idx] = next_pred
            next_pred_rescaled = scaler.inverse_transform(next_pred_full)[0, target_col_idx]
            next_predictions[name.upper()] = next_pred_rescaled
            print(f"{name.upper()}: {next_pred_rescaled:.2f}")
    
    if TENSORFLOW_AVAILABLE and 'lstm_model' in locals():
        lstm_next = lstm_model.predict(last_sequence)[0, 0]
        lstm_next_full = np.zeros((1, len(feature_columns)))
        lstm_next_full[0, target_col_idx] = lstm_next
        lstm_next_rescaled = scaler.inverse_transform(lstm_next_full)[0, target_col_idx]
        next_predictions['LSTM'] = lstm_next_rescaled
        print(f"LSTM: {lstm_next_rescaled:.2f}")
    
    # Ensemble da pr√≥xima previs√£o
    if len(next_predictions) > 1:
        ensemble_next = np.mean(list(next_predictions.values()))
        print(f"ENSEMBLE: {ensemble_next:.2f}")
    
    # Plots
    print("\n=== GERANDO GR√ÅFICOS ===")
    plot_comprehensive_results(y_test_rescaled.reshape(-1, 1), predictions, history)
    
    print("\n=== AN√ÅLISE CONCLU√çDA ===")
    print(f"Fonte dos dados: {'PostgreSQL' if use_database else 'CSV'}")
    print(f"Total de features utilizadas: {len(feature_columns)}")
    print(f"Modelos treinados: {list(predictions.keys())}")
    
    return results_df, predictions, next_predictions

if __name__ == "__main__":
    main()
