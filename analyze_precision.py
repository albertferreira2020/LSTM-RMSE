#!/usr/bin/env python3
"""
Script para an√°lise detalhada da precis√£o e confian√ßa do modelo Random Forest
Inclui m√∫ltiplas m√©tricas de avalia√ß√£o e intervalos de confian√ßa
"""

import pickle
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

from config import *
from technical_indicators import add_technical_indicators, add_lagged_features, add_rolling_statistics
from database import DatabaseManager

def prepare_all_sensor_features(df):
    """Prepara TODAS as colunas dispon√≠veis como features/sensores para previs√£o"""
    original_sensors = [
        'open', 'high', 'low', 'volume', 'reversal',
        'best_bid_price', 'best_bid_quantity', 
        'best_ask_price', 'best_ask_quantity',
        'spread', 'spread_percentage',
        'bid_liquidity', 'ask_liquidity', 'total_liquidity',
        'imbalance', 'weighted_mid_price'
    ]
    
    available_sensors = [col for col in original_sensors if col in df.columns]
    sensor_features = []
    
    # Features b√°sicas de pre√ßo
    if all(col in df.columns for col in ['open', 'high', 'low', 'close']):
        df['price_range'] = df['high'] - df['low']
        df['body_size'] = abs(df['close'] - df['open'])
        sensor_features.extend(['price_range', 'body_size'])
    
    # Features de volume
    if 'volume' in df.columns:
        df['volume_ma5'] = df['volume'].rolling(5).mean()
        sensor_features.append('volume_ma5')
    
    # Features temporais b√°sicas
    if 'close' in df.columns:
        for lag in [1, 3, 5]:
            lag_col = f'close_lag_{lag}'
            df[lag_col] = df['close'].shift(lag)
            sensor_features.append(lag_col)
    
    all_features = available_sensors + sensor_features
    return df, all_features

def parallel_feature_engineering(df, features_config):
    """Calcula indicadores t√©cnicos em paralelo"""
    df_enhanced, sensor_features = prepare_all_sensor_features(df)
    
    # Calcula indicadores t√©cnicos b√°sicos
    try:
        df_enhanced = add_technical_indicators(df_enhanced, features_config)
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao adicionar indicadores t√©cnicos: {e}")
    
    # Features lag b√°sicas
    try:
        base_columns = ['close', 'open', 'high', 'low']
        if 'volume' in df_enhanced.columns:
            base_columns.append('volume')
        
        available_base = [col for col in base_columns if col in df_enhanced.columns]
        lags = features_config.get('lag_features', [1, 2, 3])
        
        df_enhanced = add_lagged_features(df_enhanced, available_base, lags)
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao adicionar features lag: {e}")
    
    # Remove linhas com NaN
    df_enhanced = df_enhanced.dropna()
    
    # Features finais
    all_features = [col for col in df_enhanced.columns if col not in ['id', 'created_at', 'updated_at', 'timestamp']]
    
    return df_enhanced, all_features

def load_saved_model(model_path):
    """Carrega o modelo salvo"""
    try:
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        print(f"‚úÖ Modelo carregado de: {model_path}")
        return model
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelo: {e}")
        return None

def prepare_test_data_for_analysis(limit=2000):
    """Prepara dados de teste para an√°lise de precis√£o"""
    print("üìä === PREPARANDO DADOS PARA AN√ÅLISE DE PRECIS√ÉO ===")
    
    # Conecta ao banco
    db = DatabaseManager()
    
    # Carrega dados recentes para teste
    df = db.load_botbinance_data(limit=limit, order_by='id DESC')
    
    if df is None or df.empty:
        print("‚ùå Erro ao carregar dados")
        return None, None, None, None, None

    print(f"üìà Dados carregados: {len(df)} registros")
    
    # Usa a mesma engenharia de features do treinamento
    config = TECHNICAL_FEATURES_PARALLEL
    df_enhanced, all_features = parallel_feature_engineering(df, config)
    
    # Seleciona features para o modelo
    exclude_columns = ['id', 'created_at', 'updated_at', 'timestamp']
    feature_columns = [col for col in all_features if col not in exclude_columns and col in df_enhanced.columns]
    
    # Prepara dados para normaliza√ß√£o
    data = df_enhanced[feature_columns + ['close']].values
    target_idx = len(feature_columns)
    
    # Normaliza√ß√£o
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data)
    
    seq_length = SEQ_LENGTH_PARALLEL
    
    # Cria sequ√™ncias
    X_sequences = []
    y_sequences = []
    y_original = []
    
    for i in range(seq_length, len(data_scaled)):
        X_sequences.append(data_scaled[i-seq_length:i, :-1])
        y_sequences.append(data_scaled[i, target_idx])
        y_original.append(data[i, target_idx])  # Valores originais
    
    X_sequences = np.array(X_sequences)
    y_sequences = np.array(y_sequences)
    y_original = np.array(y_original)
    
    # Achata as sequ√™ncias para o Random Forest
    X_flat = X_sequences.reshape(X_sequences.shape[0], -1)
    
    print(f"üìä Shape dos dados: X_flat={X_flat.shape}, y={y_sequences.shape}")
    
    return X_flat, y_sequences, y_original, scaler, target_idx

def calculate_confidence_intervals(model, X, n_estimators=None):
    """
    Calcula intervalos de confian√ßa usando as previs√µes individuais das √°rvores
    """
    if hasattr(model, 'estimators_'):
        # Pega previs√µes de cada √°rvore individual
        tree_predictions = np.array([tree.predict(X) for tree in model.estimators_])
        
        # Calcula estat√≠sticas
        mean_pred = np.mean(tree_predictions, axis=0)
        std_pred = np.std(tree_predictions, axis=0)
        
        # Intervalos de confian√ßa (95%)
        confidence_95_lower = mean_pred - 1.96 * std_pred
        confidence_95_upper = mean_pred + 1.96 * std_pred
        
        # Intervalos de confian√ßa (68% - 1 desvio padr√£o)
        confidence_68_lower = mean_pred - std_pred
        confidence_68_upper = mean_pred + std_pred
        
        return {
            'mean': mean_pred,
            'std': std_pred,
            'confidence_95_lower': confidence_95_lower,
            'confidence_95_upper': confidence_95_upper,
            'confidence_68_lower': confidence_68_lower,
            'confidence_68_upper': confidence_68_upper
        }
    else:
        return None

def analyze_model_precision(model):
    """
    An√°lise completa da precis√£o do modelo Random Forest
    """
    print("üéØ === AN√ÅLISE DETALHADA DE PRECIS√ÉO E CONFIAN√áA ===")
    
    # 1. Prepara dados de teste
    X_test, y_test_norm, y_test_orig, scaler, target_idx = prepare_test_data_for_analysis(limit=2000)
    
    if X_test is None:
        return
    
    # 2. Faz previs√µes
    print("\nüîÆ Fazendo previs√µes para an√°lise...")
    y_pred_norm = model.predict(X_test)
    
    # 3. Desnormaliza previs√µes para valores reais
    print("üîÑ Desnormalizando previs√µes...")
    
    # Cria arrays tempor√°rios para desnormaliza√ß√£o
    temp_real = np.zeros((len(y_test_norm), scaler.n_features_in_))
    temp_pred = np.zeros((len(y_pred_norm), scaler.n_features_in_))
    
    temp_real[:, target_idx] = y_test_norm
    temp_pred[:, target_idx] = y_pred_norm
    
    # Desnormaliza (aproxima√ß√£o - pode ter pequenos erros devido ao scaler diferente)
    # Para an√°lise, usaremos os valores originais diretamente
    y_real = y_test_orig
    
    # Aproxima√ß√£o da desnormaliza√ß√£o das previs√µes
    # Calcula a escala aproximada
    real_min, real_max = y_real.min(), y_real.max()
    norm_min, norm_max = y_test_norm.min(), y_test_norm.max()
    
    # Desnormaliza as previs√µes usando escala aproximada
    y_pred_approx = y_pred_norm * (real_max - real_min) + real_min
    
    # 4. Calcula intervalos de confian√ßa
    print("üìä Calculando intervalos de confian√ßa...")
    confidence_data = calculate_confidence_intervals(model, X_test[-100:])  # √öltimas 100 previs√µes
    
    # 5. M√©tricas de precis√£o no espa√ßo normalizado
    print("\nüìà === M√âTRICAS NO ESPA√áO NORMALIZADO ===")
    rmse_norm = np.sqrt(mean_squared_error(y_test_norm, y_pred_norm))
    mae_norm = mean_absolute_error(y_test_norm, y_pred_norm)
    r2_norm = r2_score(y_test_norm, y_pred_norm)
    mape_norm = mean_absolute_percentage_error(y_test_norm, y_pred_norm) * 100
    
    print(f"üéØ RMSE (normalizado): {rmse_norm:.6f}")
    print(f"üéØ MAE (normalizado): {mae_norm:.6f}")
    print(f"üéØ R¬≤ (coef. determina√ß√£o): {r2_norm:.4f} ({r2_norm*100:.2f}%)")
    print(f"üéØ MAPE (erro % m√©dio): {mape_norm:.2f}%")
    
    # 6. M√©tricas aproximadas no espa√ßo real
    print(f"\nüí∞ === M√âTRICAS APROXIMADAS (VALORES REAIS) ===")
    rmse_real = np.sqrt(mean_squared_error(y_real[-len(y_pred_approx):], y_pred_approx))
    mae_real = mean_absolute_error(y_real[-len(y_pred_approx):], y_pred_approx)
    r2_real = r2_score(y_real[-len(y_pred_approx):], y_pred_approx)
    
    print(f"üí∞ RMSE (USD): ${rmse_real:,.2f}")
    print(f"üí∞ MAE (USD): ${mae_real:,.2f}")
    print(f"üí∞ R¬≤ (real): {r2_real:.4f} ({r2_real*100:.2f}%)")
    
    # 7. An√°lise de dire√ß√£o (alta/baixa)
    print(f"\nüìä === AN√ÅLISE DE DIRE√á√ÉO ===")
    
    # Calcula mudan√ßas de dire√ß√£o
    real_changes = np.diff(y_real[-len(y_pred_approx):])
    pred_changes = np.diff(y_pred_approx)
    
    # Classifica como alta (1) ou baixa (-1)
    real_direction = np.sign(real_changes)
    pred_direction = np.sign(pred_changes)
    
    # Accuracy direcional
    direction_accuracy = np.mean(real_direction == pred_direction) * 100
    
    print(f"üéØ Precis√£o Direcional: {direction_accuracy:.2f}%")
    print(f"   (Acerta dire√ß√£o da varia√ß√£o em {direction_accuracy:.1f}% dos casos)")
    
    # 8. Distribui√ß√£o dos erros
    errors_norm = np.abs(y_test_norm - y_pred_norm)
    errors_pct = (errors_norm / y_test_norm) * 100
    
    print(f"\nüìä === DISTRIBUI√á√ÉO DOS ERROS ===")
    print(f"üéØ Erro < 1%: {np.mean(errors_pct < 1)*100:.1f}% das previs√µes")
    print(f"üéØ Erro < 2%: {np.mean(errors_pct < 2)*100:.1f}% das previs√µes")
    print(f"üéØ Erro < 5%: {np.mean(errors_pct < 5)*100:.1f}% das previs√µes")
    print(f"üéØ Erro m√©dio: {np.mean(errors_pct):.2f}%")
    print(f"üéØ Erro mediano: {np.median(errors_pct):.2f}%")
    
    # 9. Intervalo de confian√ßa
    if confidence_data:
        print(f"\nüîí === INTERVALOS DE CONFIAN√áA ===")
        print(f"üéØ Desvio padr√£o m√©dio entre √°rvores: {np.mean(confidence_data['std']):.6f}")
        print(f"üéØ Variabilidade das previs√µes: {np.mean(confidence_data['std'])/np.mean(confidence_data['mean'])*100:.2f}%")
        
        # Analisa quantas previs√µes reais caem dentro dos intervalos
        last_100_real = y_test_norm[-100:]
        in_95_interval = np.mean(
            (last_100_real >= confidence_data['confidence_95_lower']) & 
            (last_100_real <= confidence_data['confidence_95_upper'])
        ) * 100
        
        in_68_interval = np.mean(
            (last_100_real >= confidence_data['confidence_68_lower']) & 
            (last_100_real <= confidence_data['confidence_68_upper'])
        ) * 100
        
        print(f"üéØ Valores reais dentro do intervalo 95%: {in_95_interval:.1f}%")
        print(f"üéØ Valores reais dentro do intervalo 68%: {in_68_interval:.1f}%")
    
    # 10. Informa√ß√µes do modelo
    print(f"\nüå≥ === INFORMA√á√ïES DO MODELO ===")
    if hasattr(model, 'n_estimators'):
        print(f"üå≥ N√∫mero de √°rvores: {model.n_estimators}")
    if hasattr(model, 'max_depth'):
        print(f"üìè Profundidade m√°xima: {model.max_depth}")
    if hasattr(model, 'oob_score_'):
        print(f"üéØ OOB Score: {model.oob_score_:.4f} ({model.oob_score_*100:.2f}%)")
    
    # 11. Resumo da confian√ßa
    print(f"\nüèÜ === RESUMO DE CONFIAN√áA ===")
    
    confianca_geral = (r2_norm * 0.4 + (direction_accuracy/100) * 0.3 + 
                      (np.mean(errors_pct < 2)/100) * 0.3) * 100
    
    print(f"üéØ CONFIAN√áA GERAL DO MODELO: {confianca_geral:.1f}%")
    print(f"   ‚Ä¢ R¬≤ (explica√ß√£o da vari√¢ncia): {r2_norm*100:.1f}%")
    print(f"   ‚Ä¢ Precis√£o direcional: {direction_accuracy:.1f}%") 
    print(f"   ‚Ä¢ Previs√µes com erro < 2%: {np.mean(errors_pct < 2)*100:.1f}%")
    
    if confianca_geral >= 85:
        nivel = "üü¢ EXCELENTE"
    elif confianca_geral >= 75:
        nivel = "üü° BOM"
    elif confianca_geral >= 65:
        nivel = "üü† RAZO√ÅVEL"
    else:
        nivel = "üî¥ BAIXO"
    
    print(f"\nüìä N√çVEL DE CONFIAN√áA: {nivel}")
    
    # 12. Recomenda√ß√µes de uso
    print(f"\nüí° === RECOMENDA√á√ïES DE USO ===")
    
    if confianca_geral >= 80:
        print("‚úÖ Modelo altamente confi√°vel para:")
        print("   ‚Ä¢ Previs√µes de curto prazo (1-6 horas)")
        print("   ‚Ä¢ Identifica√ß√£o de tend√™ncias")
        print("   ‚Ä¢ Suporte a decis√µes de trading")
    elif confianca_geral >= 70:
        print("‚ö†Ô∏è Modelo moderadamente confi√°vel para:")
        print("   ‚Ä¢ An√°lise de tend√™ncias gerais")
        print("   ‚Ä¢ Complemento a outras an√°lises")
        print("   ‚Ä¢ Alertas de mudan√ßas de padr√£o")
    else:
        print("‚ùå Modelo com baixa confian√ßa:")
        print("   ‚Ä¢ Use apenas como refer√™ncia")
        print("   ‚Ä¢ Combine com outras fontes")
        print("   ‚Ä¢ Considere retreinar com mais dados")
    
    return {
        'r2_score': r2_norm,
        'mae_normalized': mae_norm,
        'mape': mape_norm,
        'direction_accuracy': direction_accuracy,
        'confidence_score': confianca_geral,
        'error_distribution': {
            'under_1pct': np.mean(errors_pct < 1)*100,
            'under_2pct': np.mean(errors_pct < 2)*100,
            'under_5pct': np.mean(errors_pct < 5)*100
        }
    }

def main():
    """Fun√ß√£o principal para an√°lise de precis√£o"""
    print("üéØ === AN√ÅLISE DE PRECIS√ÉO E CONFIAN√áA DO MODELO ===")
    
    # Carrega modelo
    model_path = "models/best_rf_parallel.pkl"
    model = load_saved_model(model_path)
    if model is None:
        return
    
    # Executa an√°lise completa
    results = analyze_model_precision(model)
    
    print(f"\n‚úÖ An√°lise de precis√£o conclu√≠da!")

if __name__ == "__main__":
    main()
